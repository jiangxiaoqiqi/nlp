{"cells":[{"cell_type":"markdown","metadata":{"id":"rMv2k-eFt5e5"},"source":["### Assignment 2 - To Embed or Not to Embed\n","\n","All text from assignment template deleted to make the notebook easier to navigate\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5022,"status":"ok","timestamp":1697729163120,"user":{"displayName":"Niklas S","userId":"13395037471783967471"},"user_tz":-120},"id":"ELBc1ntbt5e8","outputId":"ea142bdc-88a6-44a5-a6f4-dccb1f3331ca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ea3cc7a0410>"]},"metadata":{},"execution_count":1}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","%matplotlib inline\n","\n","torch.manual_seed(1)"]},{"cell_type":"code","source":["#Mount GDrive to save models\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlpSvGaRD8b_","executionInfo":{"status":"ok","timestamp":1697729191630,"user_tz":-120,"elapsed":18753,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"f5376433-6d01-440c-e81a-e4b313b419d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14489,"status":"ok","timestamp":1697729206113,"user":{"displayName":"Niklas S","userId":"13395037471783967471"},"user_tz":-120},"id":"CfRAWnCvXpZF","outputId":"4a97d9b6-a506-44b6-fe32-dfa49dcd5aa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1B26XNKmZ6wS1tFR5NN3eWYBZlmnZC5gH\n","To: /content/tripadvisor_hotel_reviews.csv\n","100% 15.0M/15.0M [00:00<00:00, 131MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1vLV8H4LnT362p77v2cgLVfN5NIeDbGwx\n","To: /content/scifi.txt\n","100% 86.1M/86.1M [00:00<00:00, 121MB/s]\n"]}],"source":["#Load data\n","!gdown 1B26XNKmZ6wS1tFR5NN3eWYBZlmnZC5gH\n","!gdown 1vLV8H4LnT362p77v2cgLVfN5NIeDbGwx\n","\n","\n","import pandas as pd\n","tripadvisor=pd.read_csv('tripadvisor_hotel_reviews.csv')\n","\n","with open(f'scifi.txt')as f:\n","    scify = f.read()"]},{"cell_type":"markdown","source":["## Step 1: making example work"],"metadata":{"id":"_2ksHGBX22Ul"}},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1697700488465,"user":{"displayName":"Niklas S","userId":"13395037471783967471"},"user_tz":-120},"id":"gXbt-utMt5fA","outputId":"af0574f5-eabd-4bd5-f598-cd3d6d4d5deb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[99.98636531829834, 98.96921420097351, 97.9815821647644, 97.02028155326843, 96.08341884613037, 95.16914963722229, 94.27534770965576, 93.40070271492004, 92.54347133636475, 91.70263171195984, 90.87639713287354, 90.06389236450195, 89.26329183578491, 88.47424221038818, 87.69621682167053, 86.92822360992432, 86.16837477684021, 85.41659188270569, 84.67174315452576, 83.9335618019104]\n"]}],"source":["# Working version for the\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","\n","nltk.download('stopwords')\n","model_save_name=\"test_model_mounted.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","model_path = os.path.join(os.getcwd(), path)\n","\n","CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n","EMBEDDING_DIM = 10\n","\n","raw_text = \"\"\"We are about to study the idea of a computational process.\n","Computational processes are abstract beings that inhabit computers.\n","As they evolve, processes manipulate other abstract things called data.\n","The evolution of a process is directed by a pattern of rules\n","called a program. People create programs to direct processes. In effect,\n","we conjure the spirits of the computer with our spells\"\"\".lower().split()\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","text=remove_stopwords(raw_text)\n","text=perform_stemming(text)\n","\n","\n","# By deriving a set from `raw_text`, we deduplicate the array\n","vocab = set(text)\n","vocab_size = len(vocab)\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","data = []\n","for i in range(2, len(text) - 2):\n","    context = [text[i - 2], text[i - 1],\n","               text[i + 1], text[i + 2]]\n","    target = text[i]\n","    data.append((context, target))\n","\n","\n","\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 128)\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        out = self.linear1(embeds)\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# create your model and train.  here are some functions to help you make\n","# the data ready for use by your module\n","losses = []\n","loss_function = nn.NLLLoss()\n","model = CBOW(len(vocab), EMBEDDING_DIM)\n","optimizer = optim.SGD(model.parameters(), lr=0.005)\n","\n","\n","\n","for epoch in range(20):\n","    total_loss = 0\n","    for context, target in data:\n","\n","        # Prepare input vector\n","        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","\n","        # zero out gradients\n","        model.zero_grad()\n","\n","        # run forward pass\n","        log_probs = model(context_idxs)\n","\n","        # Compute loss function\n","        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","    losses.append(total_loss)\n","print(losses)\n","\n","torch.save(model.state_dict(), model_path)\n"]},{"cell_type":"markdown","metadata":{"id":"1U9mmH3YyQfX"},"source":["## Tripadvisor embedding CBOW2\n","\n","Embedd the words in the Hotel Review dataset with a context size of 2\n","\n"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJe84LwMyiZc","outputId":"ffa30765-9af5-48c1-8db6-0c891fb10052","executionInfo":{"status":"ok","timestamp":1697716939865,"user_tz":-120,"elapsed":23307,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}],"source":["import nltk\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","nltk.download('stopwords')\n","\n","import string\n","exclude = string.punctuation\n","\n","#Pre-processing\n","def remove_punc(text):\n","    translation_table = str.maketrans(exclude, ' ' * len(exclude))\n","    return text.translate(translation_table)\n","\n","\n","\n","def is_english_word(word):\n","    from nltk.corpus import words\n","\n","    # Make sure the word is in lowercase\n","    word = word.lower()\n","\n","    # Check if the word exists in the English words corpus\n","    return word in words.words()\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","\n","data = []\n","\n","#Loop through dataset\n","for raw_text in tripadvisor.Review:\n","    text_without_punc = remove_punc(raw_text)\n","\n","\n","    split_by_periods = text_without_punc.split(' ')\n","    # Split each resulting substring along whitespaces and \"/\"\n","    result = [substring.split() for substring in split_by_periods]\n","\n","\n","    # Flatten the list of lists to get a single list of words and phrases\n","    flat_result = [word for sublist in result for word in sublist]\n","\n","    #drop all entries that contain numbers -> mostly times, dates, room number..\n","    filtered_result = [word for word in flat_result if not any(char.isnumeric() for char in word)]\n","\n","    text_no_stop=remove_stopwords(filtered_result)\n","   #text_stemmed=perform_stemming(text_no_stop)\n","\n","\n","    # Remove all words that are not in english dictionary - removed due to very long runtime\n","    #english_words = [word for word in text_no_stop if is_english_word(word)]\n","\n","    #text=english_words\n","\n","    text=text_no_stop\n","\n","\n","\n","    #Get contexts and targets\n","    for i in range(2, len(text) - 2):\n","        context = [text[i - 2], text[i - 1],\n","                  text[i + 1], text[i + 2]]\n","        target = text[i]\n","        data.append((context, target))\n","\n","\n","unique_words = set()\n","\n","for item in data:\n","    words, _ = item\n","    unique_words.update(words)\n","\n","# Convert the set to a list to display the unique vocabulary\n","vocab = unique_words\n","vocab_size = len(vocab)\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","#Transform data to numeric\n","X, y = [], []\n","for context, target in data:\n","  X.append(context)\n","  y.append(target)\n","\n","\n","X_numeric=[]\n","y_numeric=[]\n","for i in X:\n","  X_numeric.append([word_to_ix[w] for w in i])\n","print(X_numeric)\n","X_train=torch.FloatTensor(X_numeric)\n","\n","y_numeric=[]\n","for i in y:\n","  y_numeric.append(word_to_ix[i])\n","print(y_numeric)\n","y_train=torch.FloatTensor(y_numeric)\n","\n","\n","\n","\n","num_batches=12000\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","#Load data to dataloader\n","class MyDataset(IterableDataset):\n","    def __init__(self, data_X, data_y):\n","        assert len(data_X) == len(data_y)\n","        self.data_X = data_X.to(device)\n","        self.data_y = data_y.to(device)\n","\n","    def __len__(self):\n","        return len(self.data_X)\n","\n","    def __iter__(self):\n","        for i in range(len(self.data_X)):\n","            yield (self.data_X[i], self.data_y[i])\n","\n","train_set = MyDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=num_batches)\n"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1067309,"status":"ok","timestamp":1697718025606,"user":{"displayName":"Niklas S","userId":"13395037471783967471"},"user_tz":-120},"id":"5HSHhhwlySLz","outputId":"586a0045-024c-4840-96d9-9f13ec8e6f70"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-60-259aea352da9>:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","<ipython-input-60-259aea352da9>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/15], loss: 1772.2735004425049]\n","Epoch [2/15], loss: 1758.4999828338623]\n","Epoch [3/15], loss: 1744.860345840454]\n","Epoch [4/15], loss: 1731.2777700424194]\n","Epoch [5/15], loss: 1717.7436351776123]\n","Epoch [6/15], loss: 1704.4070415496826]\n","Epoch [7/15], loss: 1691.6781911849976]\n","Epoch [8/15], loss: 1680.0424394607544]\n","Epoch [9/15], loss: 1669.524401664734]\n","Epoch [10/15], loss: 1659.7103281021118]\n","Epoch [11/15], loss: 1650.2076044082642]\n","Epoch [12/15], loss: 1640.8156633377075]\n","Epoch [13/15], loss: 1631.5071926116943]\n","Epoch [14/15], loss: 1622.356206893921]\n","Epoch [15/15], loss: 1613.4701385498047]\n","[1772.2735004425049, 1758.4999828338623, 1744.860345840454, 1731.2777700424194, 1717.7436351776123, 1704.4070415496826, 1691.6781911849976, 1680.0424394607544, 1669.524401664734, 1659.7103281021118, 1650.2076044082642, 1640.8156633377075, 1631.5071926116943, 1622.356206893921, 1613.4701385498047]\n"]}],"source":["#Train the model\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","\n","\n","num_epochs=15\n","# Source: https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n","CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n","EMBEDDING_DIM = 50\n","\n","model_save_name=\"tripadvisor_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","model_path = os.path.join(os.getcwd(), path)\n","\n","\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","\n","        embeds = self.embeddings(inputs).sum(dim=1)\n","\n","        out = self.linear1(embeds)\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# create your model and train.  here are some functions to help you make\n","# the data ready for use by your module\n","losses = []\n","loss_function = nn.NLLLoss()\n","model = CBOW(len(vocab), EMBEDDING_DIM)\n","model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.005)\n","\n","\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    counter=0\n","    for batch in train_loader:\n","        counter=counter+1\n","\n","        context=batch[0]\n","        target=batch[1]\n","        counter=counter+1\n","\n","\n","        # Prepare input vector\n","        context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","\n","        # zero out gradients\n","        model.zero_grad()\n","\n","        # run forward pass\n","        log_probs = model(context_idxs)\n","\n","        # Compute loss function\n","        loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","    losses.append(total_loss)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], loss: {total_loss}]')\n","print(losses)\n","\n","torch.save(model.state_dict(), path)\n","\n"]},{"cell_type":"markdown","source":["**Make Predictions**\n","To check whether the embedding makes sense, we test it on some self-defined context words\n","\n"],"metadata":{"id":"7F3EmI8EM7hk"}},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1697708413038,"user":{"displayName":"Niklas S","userId":"13395037471783967471"},"user_tz":-120},"id":"pGFMoAgwmaLP","outputId":"c036d2b5-7593-473b-98b1-d7204fe0beb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","Context: ['view', 'bath', 'great', 'beach'], Predicted word: hotel\n","torch.Size([4])\n","Context: ['food', 'bad', 'disappoint', 'waiter'], Predicted word: hotel\n","torch.Size([4])\n","Context: ['beach', 'swim', 'bad', 'cold'], Predicted word: room\n","torch.Size([4])\n","Context: ['scienc', 'run', 'write', 'help'], Predicted word: hotel\n","torch.Size([4])\n","Context: ['bed', 'qualiti', 'sleep', 'dream'], Predicted word: hotel\n","torch.Size([4])\n","Context: ['pizza', 'pasta', 'fri', 'potato'], Predicted word: room\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import os\n","import torch\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n","\n","model_save_name=\"tripadvisor_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        print(inputs.shape)\n","        embeds = self.embeddings(inputs).sum(dim=0)\n","\n","        out = self.linear1(embeds)\n","\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# Load the trained model\n","\n","model_path = os.path.join(path)  # Replace with the path to your saved model\n","model = CBOW(vocab_size, 50)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","model.eval()  # Set the model to evaluation mode\n","\n","# Function to predict the next word\n","def predict_next_word(context, model):\n","    context_tokens = word_tokenize(context)  # Tokenize the context\n","    if len(context_tokens) < 2 * CONTEXT_SIZE:\n","        return \"Context is too short\"\n","\n","    # Extract the context window of size CONTEXT_SIZE\n","    context_window = context_tokens[-CONTEXT_SIZE:] + context_tokens[:CONTEXT_SIZE]\n","\n","    # Convert context to a list of word indices\n","    context_indices = [word_to_ix[word] for word in context_window]\n","\n","    # Prepare input tensor\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long).to(device)\n","\n","\n","    # Run forward pass\n","    with torch.no_grad():\n","        log_probs = model(context_tensor)\n","\n","    # Get the index of the most likely word\n","    predicted_word_index = torch.argmax(log_probs).item()\n","\n","\n","    # Map the index back to the word\n","    predicted_word = ix_to_word[predicted_word_index]\n","\n","    return predicted_word\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","# Example usage\n","for context in [\"view bath great beach\", \"food bad disappointing waiter\",\"beach swim bad cold\", \"science running writing helping\",\"bed quality sleep dream\",\"pizza pasta fries potato\"]:\n","\n","\n","  context=perform_stemming(context.split(\" \"))\n","  stemmed_context=\"\"\n","\n","  for x in context:\n","    stemmed_context+=x+\" \"\n","  stemmed_context\n","\n","\n","\n","  predicted_word = predict_next_word(stemmed_context, model)\n","  print(f'Context: {context}, Predicted word: {predicted_word}')"]},{"cell_type":"markdown","source":["## Tripadvisor CBOW5"],"metadata":{"id":"pJoYrB2IM-N1"}},{"cell_type":"code","source":["#Pre-process and load to dataloader\n","import nltk\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","nltk.download('stopwords')\n","\n","import string\n","exclude = string.punctuation\n","\n","#nltk.download('words')\n","def remove_punc(text):\n","    translation_table = str.maketrans(exclude, ' ' * len(exclude))\n","    return text.translate(translation_table)\n","\n","\n","\n","def is_english_word(word):\n","    from nltk.corpus import words\n","\n","    # Make sure the word is in lowercase\n","    word = word.lower()\n","\n","    # Check if the word exists in the English words corpus\n","    return word in words.words()\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","\n","data = []\n","\n","#Loop through dataset\n","for raw_text in tripadvisor.Review:\n","    text_without_punc = remove_punc(raw_text)\n","\n","    #split text to list\n","    split_by_periods = text_without_punc.split(' ')\n","    # Split each resulting substring along whitespaces and \"/\"\n","    result = [substring.split() for substring in split_by_periods]\n","\n","\n","    # Flatten the list of lists to get a single list of words and phrases\n","    flat_result = [word for sublist in result for word in sublist]\n","\n","    #drop all entries that contain numbers -> mostly times, dates, room number..\n","    filtered_result = [word for word in flat_result if not any(char.isnumeric() for char in word)]\n","\n","    text_no_stop=remove_stopwords(filtered_result)\n","    #text_stemmed=perform_stemming(text_no_stop)\n","\n","\n","    # Remove all words that are not in english dictionary - removed due to very long runtime\n","    #english_words = [word for word in text_no_stop if is_english_word(word)]\n","\n","    #text=english_words\n","\n","    text=text_no_stop\n","\n","\n","\n","    for i in range(5, len(text) - 5):\n","        context = [text[i - 5],\n","                  text[i - 4], text[i - 3],\n","                  text[i - 2], text[i - 1],\n","                  text[i + 1], text[i + 2],\n","                  text[i + 3], text[i +4],\n","                  text[i+5]]\n","        target = text[i]\n","        data.append((context, target))\n","\n","\n","unique_words = set()\n","\n","for item in data:\n","    words, _ = item\n","    unique_words.update(words)\n","\n","# Convert the set to a list to display the unique vocabulary\n","vocab = unique_words\n","vocab_size = len(vocab)\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","X, y = [], []\n","for context, target in data:\n","  X.append(context)\n","  y.append(target)\n","\n","\n","X_numeric=[]\n","y_numeric=[]\n","for i in X:\n","  X_numeric.append([word_to_ix[w] for w in i])\n","print(X_numeric)\n","X_train=torch.FloatTensor(X_numeric)\n","\n","y_numeric=[]\n","for i in y:\n","  y_numeric.append(word_to_ix[i])\n","print(y_numeric)\n","y_train=torch.FloatTensor(y_numeric)\n","\n","\n","\n","\n","num_batches=6000\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","class MyDataset(IterableDataset):\n","    def __init__(self, data_X, data_y):\n","        assert len(data_X) == len(data_y)\n","        self.data_X = data_X.to(device)\n","        self.data_y = data_y.to(device)\n","\n","    def __len__(self):\n","        return len(self.data_X)\n","\n","    def __iter__(self):\n","        for i in range(len(self.data_X)):\n","            yield (self.data_X[i], self.data_y[i])\n","\n","train_set = MyDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=num_batches)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkM0sCPKNBRI","executionInfo":{"status":"ok","timestamp":1697715166098,"user_tz":-120,"elapsed":64867,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"2b4656c8-5fda-4a0b-aa31-537f1c567df0"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","\n","num_epochs=15\n","\n","# Source: https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n","CONTEXT_SIZE = 5 # 5 words to the left, 2 to the right\n","EMBEDDING_DIM = 50\n","\n","model_save_name=\"tripadvisor_CBOW5.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","model_path = os.path.join(os.getcwd(), path)\n","\n","\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","\n","        embeds = self.embeddings(inputs).sum(dim=1)\n","\n","        out = self.linear1(embeds)\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# create your model and train.  here are some functions to help you make\n","# the data ready for use by your module\n","losses = []\n","loss_function = nn.NLLLoss()\n","model = CBOW(len(vocab), EMBEDDING_DIM)\n","model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.005)\n","\n","\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    counter=0\n","    for batch in train_loader:\n","        counter=counter+1\n","\n","        context=batch[0]\n","        target=batch[1]\n","        counter=counter+1\n","\n","\n","        # Prepare input vector\n","        context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","\n","        # zero out gradients\n","        model.zero_grad()\n","\n","        # run forward pass\n","        log_probs = model(context_idxs)\n","\n","        # Compute loss function\n","        loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","    losses.append(total_loss)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], loss: {total_loss}]')\n","print(losses)\n","\n","torch.save(model.state_dict(), path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edyHRU_w6LKI","executionInfo":{"status":"ok","timestamp":1697709285902,"user_tz":-120,"elapsed":679354,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"dd774f5f-7117-4dfb-b71c-1c053c314f57"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-31-59cf360a8157>:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","<ipython-input-31-59cf360a8157>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/15], loss: 3208.8699626922607]\n","Epoch [2/15], loss: 3066.107057571411]\n","Epoch [3/15], loss: 2968.185814857483]\n","Epoch [4/15], loss: 2888.826210975647]\n","Epoch [5/15], loss: 2823.719162940979]\n","Epoch [6/15], loss: 2771.010648727417]\n","Epoch [7/15], loss: 2727.4765796661377]\n","Epoch [8/15], loss: 2690.543646812439]\n","Epoch [9/15], loss: 2658.7761278152466]\n","Epoch [10/15], loss: 2631.071268081665]\n","Epoch [11/15], loss: 2606.6073932647705]\n","Epoch [12/15], loss: 2584.7755184173584]\n","Epoch [13/15], loss: 2565.126700401306]\n","Epoch [14/15], loss: 2547.345413684845]\n","Epoch [15/15], loss: 2531.1996302604675]\n","[3208.8699626922607, 3066.107057571411, 2968.185814857483, 2888.826210975647, 2823.719162940979, 2771.010648727417, 2727.4765796661377, 2690.543646812439, 2658.7761278152466, 2631.071268081665, 2606.6073932647705, 2584.7755184173584, 2565.126700401306, 2547.345413684845, 2531.1996302604675]\n"]}]},{"cell_type":"markdown","source":["**Make Predictions**\n","To check whether the embedding makes sense, we test it on some self-defined context words\n","\n"],"metadata":{"id":"e9iq-P1fuem1"}},{"cell_type":"code","source":["import os\n","import torch\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n","\n","model_save_name=\"tripadvisor_CBOW5.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        print(inputs.shape)\n","        embeds = self.embeddings(inputs).sum(dim=0)\n","\n","        out = self.linear1(embeds)\n","\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# Load the trained model\n","\n","model_path = os.path.join(path)  # Replace with the path to your saved model\n","model = CBOW(vocab_size, 50)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","model.eval()  # Set the model to evaluation mode\n","\n","# Function to predict the next word\n","def predict_next_word(context, model):\n","    context_tokens = word_tokenize(context)  # Tokenize the context\n","    if len(context_tokens) < 2 * CONTEXT_SIZE:\n","        return \"Context is too short\"\n","\n","    # Extract the context window of size CONTEXT_SIZE\n","    context_window = context_tokens[-CONTEXT_SIZE:] + context_tokens[:CONTEXT_SIZE]\n","\n","    # Convert context to a list of word indices\n","    context_indices = [word_to_ix[word] for word in context_window]\n","\n","    # Prepare input tensor\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long).to(device)\n","\n","\n","    # Run forward pass\n","    with torch.no_grad():\n","        log_probs = model(context_tensor)\n","\n","    # Get the index of the most likely word\n","    predicted_word_index = torch.argmax(log_probs).item()\n","\n","\n","    # Map the index back to the word\n","    predicted_word = ix_to_word[predicted_word_index]\n","\n","    return predicted_word\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","# Example usage\n","for context in [\"view bath great beach swimming house lake water warm great\", \"pasta pizza seafood good potato fries rice tomato view table\", \"hard sleep uncomfortable loud blanket pillow bad noisy good tired\", \"said reception check-in late fast good complicated wait lovely experience room\"]:\n","\n","\n","  context=perform_stemming(context.split(\" \"))\n","  stemmed_context=\"\"\n","\n","  for x in context:\n","    stemmed_context+=x+\" \"\n","  stemmed_context\n","\n","\n","\n","  predicted_word = predict_next_word(stemmed_context, model)\n","  print(f'Context: {context}, Predicted word: {predicted_word}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTycZmDkUPV5","executionInfo":{"status":"ok","timestamp":1697715446670,"user_tz":-120,"elapsed":406,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"d3849182-e305-4382-eff8-007df1202c4a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","Context: ['view', 'bath', 'great', 'beach', 'swim', 'hous', 'lake', 'water', 'warm', 'great'], Predicted word: beachrestaraunt\n","torch.Size([4])\n","Context: ['pasta', 'pizza', 'seafood', 'good', 'potato', 'fri', 'rice', 'tomato', 'view', 'tabl'], Predicted word: mirador\n","torch.Size([4])\n","Context: ['hard', 'sleep', 'uncomfort', 'loud', 'blanket', 'pillow', 'bad', 'noisi', 'good', 'tire'], Predicted word: beachrestaraunt\n","torch.Size([4])\n","Context: ['said', 'recept', 'check-in', 'late', 'fast', 'good', 'complic', 'wait', 'love', 'experi', 'room'], Predicted word: mirador\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Scifiy Text"],"metadata":{"id":"bknU2IE514V1"}},{"cell_type":"code","source":["#Preprocess data and load to dataloader\n","import nltk\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","nltk.download('stopwords')\n","\n","import string\n","exclude = string.punctuation\n","\n","def remove_punc(text):\n","    return text.translate(str.maketrans('', '', exclude))\n","\n","\n","#nltk.download('words')\n","\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","\n","data = []\n","\n","raw_text=scify.lower()\n","\n","text_without_punc = remove_punc(raw_text)\n","\n","chunk_size = 500\n","\n","chunks = [text_without_punc[i:i+chunk_size] for i in range(0, len(text_without_punc), chunk_size)]\n","\n","\n","print(text_without_punc[:400])\n","\n","#\n","#split_by_periods = raw_text.split('.')\n","\n","\n","# Split each resulting substring along whitespaces and \"/\"\n","result = [substring.split() for substring in chunks]\n","\n","\n","#Flatten the list of lists to get a single list of words and phrases\n","flat_result = [word for sublist in result for word in sublist]\n","\n","#drop all entries that contain numbers -> mostly times, dates, room number..\n","filtered_result = [word for word in flat_result if not any(char.isnumeric() for char in word)]\n","\n","text_no_stop=remove_stopwords(filtered_result)\n","#text_stemmed=perform_stemming(text_no_stop)\n","\n","\n","# Only keep word that exist in the english lanugage\n","#english_words = [word for word in text_no_stop if is_english_word(word)]\n","\n","#text=english_words\n","\n","text=text_no_stop\n","\n","\n","\n","\n","\n","\n","for i in range(2, len(text) - 2):\n","    context = [text[i - 2], text[i - 1],\n","              text[i + 1], text[i + 2]]\n","    target = text[i]\n","    data.append((context, target))\n","\n","\n","unique_words = set()\n","\n","for item in data:\n","    words, _ = item\n","    unique_words.update(words)\n","\n","# Convert the set to a list to display the unique vocabulary\n","vocab = unique_words\n","vocab_size = len(vocab)\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","\n","\n","X, y = [], []\n","for context, target in data:\n","  X.append(context)\n","  y.append(target)\n","\n","\n","X_numeric=[]\n","y_numeric=[]\n","for i in X:\n","  X_numeric.append([word_to_ix[w] for w in i])\n","print(X_numeric)\n","X_train=torch.FloatTensor(X_numeric)\n","\n","y_numeric=[]\n","for i in y:\n","  y_numeric.append(word_to_ix[i])\n","print(y_numeric)\n","y_train=torch.FloatTensor(y_numeric)\n","\n","\n","\n","\n","num_batches=3000\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","class MyDataset(IterableDataset):\n","    def __init__(self, data_X, data_y):\n","        assert len(data_X) == len(data_y)\n","        self.data_X = data_X.to(device)\n","        self.data_y = data_y.to(device)\n","\n","    def __len__(self):\n","        return len(self.data_X)\n","\n","    def __iter__(self):\n","        for i in range(len(self.data_X)):\n","            yield (self.data_X[i], self.data_y[i])\n","\n","train_set = MyDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=num_batches)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dhzn5k6i17Gg","executionInfo":{"status":"ok","timestamp":1697722211179,"user_tz":-120,"elapsed":99480,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"c025c0d5-ce83-4311-9db2-ae4905d38c27"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc kingston new york volume  no  copyright  by quinn publishing company inc application for entry as second class matter at post office buffalo new york pending subscription  for  issues in us and possessions canada  for  issues elsewhere  aiiow four weeks for change of address all stories a\n"]},{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["#Train Model\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","\n","num_epochs=3\n","\n","# Source: https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n","CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n","EMBEDDING_DIM = 50\n","\n","model_save_name=\"scify_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","model_path = os.path.join(os.getcwd(), path)\n","\n","\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","\n","        embeds = self.embeddings(inputs).sum(dim=1)\n","\n","        out = self.linear1(embeds)\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# create your model and train.  here are some functions to help you make\n","# the data ready for use by your module\n","losses = []\n","loss_function = nn.NLLLoss()\n","model = CBOW(len(vocab), EMBEDDING_DIM)\n","model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.005)\n","\n","\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    counter=0\n","    for batch in train_loader:\n","        counter=counter+1\n","\n","        context=batch[0]\n","        target=batch[1]\n","        counter=counter+1\n","\n","\n","        # Prepare input vector\n","        context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","\n","        # zero out gradients\n","        model.zero_grad()\n","\n","        # run forward pass\n","        log_probs = model(context_idxs)\n","\n","        # Compute loss function\n","        loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","    losses.append(total_loss)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], loss: {total_loss}]')\n","print(losses)\n","\n","torch.save(model.state_dict(), path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSUAVp0rH9ux","executionInfo":{"status":"ok","timestamp":1697725607124,"user_tz":-120,"elapsed":3136122,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"4d9b891a-ada1-42bb-ec73-219b12505993"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-49434899bb0d>:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  context_idxs = torch.tensor(context, dtype=torch.long).to(device)\n","<ipython-input-6-49434899bb0d>:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = loss_function(log_probs, torch.tensor(target, dtype=torch.long).to(device))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], loss: 32425.964317321777]\n","Epoch [2/3], loss: 31331.134305000305]\n","Epoch [3/3], loss: 30287.25228023529]\n","[32425.964317321777, 31331.134305000305, 30287.25228023529]\n"]}]},{"cell_type":"markdown","source":["**Make Predictions**\n","To check whether the embedding makes sense, we test it on some self-defined context words"],"metadata":{"id":"jW27xN_sup1I"}},{"cell_type":"code","source":["import os\n","import torch\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n","\n","model_save_name=\"scify_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        print(inputs.shape)\n","        embeds = self.embeddings(inputs).sum(dim=0)\n","\n","        out = self.linear1(embeds)\n","\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# Load the trained model\n","\n","model_path = os.path.join(path)  # Replace with the path to your saved model\n","model = CBOW(vocab_size, 50)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","model.eval()  # Set the model to evaluation mode\n","\n","# Function to predict the next word\n","def predict_next_word(context, model):\n","    context_tokens = word_tokenize(context)  # Tokenize the context\n","    if len(context_tokens) < 2 * CONTEXT_SIZE:\n","        return \"Context is too short\"\n","\n","    # Extract the context window of size CONTEXT_SIZE\n","    context_window = context_tokens[-CONTEXT_SIZE:] + context_tokens[:CONTEXT_SIZE]\n","\n","    # Convert context to a list of word indices\n","    context_indices = [word_to_ix[word] for word in context_window]\n","\n","    # Prepare input tensor\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long).to(device)\n","\n","\n","    # Run forward pass\n","    with torch.no_grad():\n","        log_probs = model(context_tensor)\n","\n","    # Get the index of the most likely word\n","    predicted_word_index = torch.argmax(log_probs).item()\n","\n","\n","    # Map the index back to the word\n","    predicted_word = ix_to_word[predicted_word_index]\n","\n","    return predicted_word\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","# Example usage\n","#for context in [\"view bath great beach\", \"food bad disappointing waiter\",\"beach swim bad cold\", \"science running writing helping\",\"bed quality sleep dream\",\"pizza pasta fries potato\"]:\n","for context in [\"flying alien rocket planet\", \"table food potato carrot\", 'space sky star planet','assignment hard model run']:\n","\n","  context=perform_stemming(context.split(\" \"))\n","  stemmed_context=\"\"\n","\n","  for x in context:\n","    stemmed_context+=x+\" \"\n","  stemmed_context\n","\n","\n","\n","  predicted_word = predict_next_word(stemmed_context, model)\n","  print(f'Context: {context}, Predicted word: {predicted_word}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTaOD2kJH4kc","executionInfo":{"status":"ok","timestamp":1697715089698,"user_tz":-120,"elapsed":840,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"98f456b1-940d-4aa9-8d7e-d80795822b23"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","Context: ['fli', 'alien', 'rocket', 'planet'], Predicted word: like\n","torch.Size([4])\n","Context: ['tabl', 'food', 'potato', 'carrot'], Predicted word: said\n","torch.Size([4])\n","Context: ['space', 'sky', 'star', 'planet'], Predicted word: said\n","torch.Size([4])\n","Context: ['assign', 'hard', 'model', 'run'], Predicted word: said\n"]}]},{"cell_type":"markdown","source":["## Closest neighbours Tripadvisor CBOW2"],"metadata":{"id":"5U8Z80sZuvQ9"}},{"cell_type":"code","source":["# Read and Preprocess (if this codeblock is run in isolation)\n","import nltk\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","nltk.download('stopwords')\n","\n","import string\n","exclude = string.punctuation\n","\n","#nltk.download('words')\n","def remove_punc(text):\n","    translation_table = str.maketrans(exclude, ' ' * len(exclude))\n","    return text.translate(translation_table)\n","\n","\n","\n","def is_english_word(word):\n","    from nltk.corpus import words\n","\n","    # Make sure the word is in lowercase\n","    word = word.lower()\n","\n","    # Check if the word exists in the English words corpus\n","    return word in words.words()\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","\n","data = []\n","for raw_text in tripadvisor.Review:\n","    text_without_punc = remove_punc(raw_text)\n","\n","\n","    split_by_periods = text_without_punc.split(' ')\n","    # Split each resulting substring along whitespaces and \"/\"\n","    result = [substring.split() for substring in split_by_periods]\n","\n","\n","    # Flatten the list of lists to get a single list of words and phrases\n","    flat_result = [word for sublist in result for word in sublist]\n","\n","    #drop all entries that contain numbers -> mostly times, dates, room number..\n","    filtered_result = [word for word in flat_result if not any(char.isnumeric() for char in word)]\n","\n","    text_no_stop=remove_stopwords(filtered_result)\n","    #text_stemmed=perform_stemming(text_no_stop)\n","\n","\n","    # Remove all words that are not in english dictionary - removed due to very long runtime\n","    #english_words = [word for word in text_no_stop if is_english_word(word)]\n","\n","    #text=english_words\n","\n","    text=text_no_stop\n","\n","\n","\n","    for i in range(2, len(text) - 2):\n","        context = [text[i - 2], text[i - 1],\n","                  text[i + 1], text[i + 2]]\n","        target = text[i]\n","        data.append((context, target))\n","\n","\n","unique_words = set()\n","\n","for item in data:\n","    words, _ = item\n","    unique_words.update(words)\n","\n","# Convert the set to a list to display the unique vocabulary\n","vocab = unique_words\n","vocab_size = len(vocab)\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","X, y = [], []\n","for context, target in data:\n","  X.append(context)\n","  y.append(target)\n","\n","\n","X_numeric=[]\n","y_numeric=[]\n","for i in X:\n","  X_numeric.append([word_to_ix[w] for w in i])\n","print(X_numeric)\n","X_train=torch.FloatTensor(X_numeric)\n","\n","y_numeric=[]\n","for i in y:\n","  y_numeric.append(word_to_ix[i])\n","print(y_numeric)\n","y_train=torch.FloatTensor(y_numeric)\n","\n","\n","\n","\n","num_batches=12000\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","class MyDataset(IterableDataset):\n","    def __init__(self, data_X, data_y):\n","        assert len(data_X) == len(data_y)\n","        self.data_X = data_X.to(device)\n","        self.data_y = data_y.to(device)\n","\n","    def __len__(self):\n","        return len(self.data_X)\n","\n","    def __iter__(self):\n","        for i in range(len(self.data_X)):\n","            yield (self.data_X[i], self.data_y[i])\n","\n","train_set = MyDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=num_batches)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVheDjdhxSdZ","executionInfo":{"status":"ok","timestamp":1697726839949,"user_tz":-120,"elapsed":19692,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"d9b26e0f-1475-428c-fa44-bf66cce23495"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["# load model\n","ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n","word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n","\n","model_save_name=\"tripadvisor_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        print(inputs.shape)\n","        embeds = self.embeddings(inputs).sum(dim=0)\n","\n","        out = self.linear1(embeds)\n","\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# Load the trained model\n","\n","model_path = os.path.join(path)  # Replace with the path to your saved model\n","model = CBOW(vocab_size, 50)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gfeqKkAxDOK","executionInfo":{"status":"ok","timestamp":1697726896559,"user_tz":-120,"elapsed":505,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"d27d210f-c79a-4bda-973d-4b39d5c2c684"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CBOW(\n","  (embeddings): Embedding(49093, 50)\n","  (linear1): Linear(in_features=50, out_features=200, bias=True)\n","  (linear2): Linear(in_features=200, out_features=49093, bias=True)\n","  (nonlin_1): ReLU()\n","  (nonlin_2): LogSoftmax(dim=-1)\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#define closest neighbor\n","import torch.nn as nn\n","import torch\n","\n","def get_closest_word(word,net, topn=5):\n","  word_distance=[]\n","  emb=net.embeddings\n","  pdist=nn.PairwiseDistance()\n","  i=word_to_ix[word]\n","\n","  lookup_tensor_i=torch.tensor([i], dtype=torch.long).to(device)\n","\n","  v_i =emb(lookup_tensor_i)\n","\n","  for j in range(len(vocab)):\n","    if j != i:\n","      lookup_tensor_j=torch.tensor([j], dtype=torch.long).to(device)\n","      v_j = emb(lookup_tensor_j)\n","      word_distance.append((ix_to_word[j], float(pdist(v_i,v_j))))\n","  word_distance.sort(key=lambda x:x[1])\n","  return word_distance"],"metadata":{"id":"MyhhxNcvvOTj","executionInfo":{"status":"ok","timestamp":1697726901062,"user_tz":-120,"elapsed":4,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Get closest neighbors for 9 words\n","for i in ['staff', 'night', 'tableware','great','clean','immature', 'walk','like','appreciate']:\n","  print(i)\n","  print(get_closest_word(i, model)[:10])\n","  print('-----------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86nZTlkVwtoY","executionInfo":{"status":"ok","timestamp":1697718867699,"user_tz":-120,"elapsed":58448,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"49514a5b-9f84-450e-fe34-c48e81002063"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["staff\n","[('neck', 6.611093044281006), ('niche', 7.048586845397949), ('properly', 7.190108299255371), ('unroll', 7.377197265625), ('barbque', 7.398947238922119), ('corenr', 7.472538471221924), ('restrict', 7.5200347900390625), ('atmasphere', 7.5293474197387695), ('cliff', 7.629973888397217), ('candies', 7.659377574920654)]\n","-----------------------------------\n","night\n","[('disk', 6.037545204162598), ('charisma', 6.218534469604492), ('grotty', 6.5701704025268555), ('washroom', 6.7157392501831055), ('resrevations', 6.723171234130859), ('awfull', 6.727161407470703), ('jammed', 6.730695724487305), ('counterparts', 6.742620944976807), ('lacked', 6.764827251434326), ('vaca', 6.77960729598999)]\n","-----------------------------------\n","tableware\n","[('setups', 6.716906547546387), ('exercised', 6.754449367523193), ('spetacular', 6.83306360244751), ('staringout', 6.906892776489258), ('united', 6.92319917678833), ('acknowledging', 6.930212020874023), ('opening', 6.979665279388428), ('calls', 6.981204986572266), ('kilter', 6.981340408325195), ('stew', 6.982442855834961)]\n","-----------------------------------\n","great\n","[('pastries', 7.350565433502197), ('nerve', 7.439746856689453), ('unrelenting', 7.535068988800049), ('desperation', 7.556973457336426), ('eperience', 7.6245598793029785), ('receptionists', 7.691327095031738), ('sewage', 7.734336853027344), ('kilometres', 7.740493297576904), ('taman', 7.7861528396606445), ('luden', 7.812316417694092)]\n","-----------------------------------\n","clean\n","[('teddy', 6.385136127471924), ('salvador', 6.451722145080566), ('upsetting', 6.681004047393799), ('flier', 6.800795078277588), ('soemthing', 6.832270622253418), ('drying', 6.853837490081787), ('rufused', 6.877006530761719), ('\\x96', 6.934366226196289), ('partied', 6.956801414489746), ('zagat', 6.9615092277526855)]\n","-----------------------------------\n","immature\n","[('outnumber', 6.1480793952941895), ('woodwork', 6.263677597045898), ('nanos', 6.408920764923096), ('tome', 6.460483074188232), ('corresponded', 6.521891117095947), ('normandie', 6.558872699737549), ('carbonation', 6.599054336547852), ('unwrinkle', 6.647782325744629), ('nurtured', 6.65819787979126), ('husbnd', 6.687697887420654)]\n","-----------------------------------\n","walk\n","[('teenaged', 6.231412410736084), ('mosquitos', 6.57386589050293), ('chuo', 6.601217746734619), ('tien', 6.639698505401611), ('tussauds', 6.6511616706848145), ('samoses', 6.653146743774414), ('alyssa', 6.688689231872559), ('restautant', 6.726736545562744), ('fifht', 6.729983806610107), ('tastelss', 6.730241298675537)]\n","-----------------------------------\n","like\n","[('quinz', 6.608607769012451), ('hovels', 6.673440933227539), ('checked', 6.700855731964111), ('tempremental', 6.716300010681152), ('wouild', 6.75217866897583), ('sants', 6.767919063568115), ('cudos', 6.774375915527344), ('clusterof', 6.790627956390381), ('dramamine', 6.808435916900635), ('globali', 6.837310791015625)]\n","-----------------------------------\n","appreciate\n","[('materialised', 6.243075847625732), ('samples', 6.330572128295898), ('eddy', 6.362059593200684), ('diabolical', 6.466047286987305), ('inclimate', 6.518741130828857), ('frenchtoast', 6.682906150817871), ('faultless', 6.693512916564941), ('stazione', 6.728541374206543), ('kioskos', 6.731070518493652), ('yan', 6.732369899749756)]\n","-----------------------------------\n"]}]},{"cell_type":"code","source":["# Get closest neighbors for 1 words\n","\n","for i in ['room', 'bed']:\n","  print(i)\n","  print(get_closest_word(i, model)[:10])\n","  print('-----------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73YcUZYsZcvJ","executionInfo":{"status":"ok","timestamp":1697726915138,"user_tz":-120,"elapsed":12323,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"e60b826f-a72a-4624-fda2-a85403a7e34a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["room\n","[('allcomers', 6.662619113922119), ('leidespein', 6.691622257232666), ('okey', 6.81837272644043), ('goodlooking', 6.867095470428467), ('cozza', 6.874899387359619), ('gentleman', 6.879683017730713), ('tui', 6.95367956161499), ('sherlock', 6.963742256164551), ('smells', 6.978347301483154), ('scrubbing', 7.022211074829102)]\n","-----------------------------------\n","bed\n","[('crystallline', 7.088943004608154), ('stevie', 7.132186412811279), ('filmed', 7.276320457458496), ('chow', 7.277835369110107), ('flow', 7.282066345214844), ('harrahs', 7.3771257400512695), ('mardis', 7.397796154022217), ('gentillesse', 7.400918006896973), ('dune', 7.474765777587891), ('compermise', 7.478332042694092)]\n","-----------------------------------\n"]}]},{"cell_type":"markdown","source":["## Embedding Test Sci-fi"],"metadata":{"id":"zPMd5-s7BwYe"}},{"cell_type":"code","source":["# Read and Preprocess (if this codeblock is run in isolation)\n","import nltk\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from torch.utils.data import IterableDataset, DataLoader\n","nltk.download('stopwords')\n","\n","import string\n","exclude = string.punctuation\n","\n","def remove_punc(text):\n","    return text.translate(str.maketrans('', '', exclude))\n","\n","\n","#nltk.download('words')\n","\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","stopwords_english = stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    new_text = []\n","    for word in text:\n","        if word in stopwords_english:\n","            continue\n","        else:\n","            new_text.append(word)\n","\n","    return new_text\n","\n","\n","ps = PorterStemmer()\n","def perform_stemming(text):\n","    new_text = [ps.stem(word) for word in text]\n","    return new_text\n","\n","\n","data = []\n","\n","raw_text=scify.lower()\n","\n","text_without_punc = remove_punc(raw_text)\n","\n","chunk_size = 500\n","\n","chunks = [text_without_punc[i:i+chunk_size] for i in range(0, len(text_without_punc), chunk_size)]\n","\n","\n","print(text_without_punc[:400])\n","\n","#\n","#split_by_periods = raw_text.split('.')\n","\n","\n","# Split each resulting substring along whitespaces and \"/\"\n","result = [substring.split() for substring in chunks]\n","\n","\n","#Flatten the list of lists to get a single list of words and phrases\n","flat_result = [word for sublist in result for word in sublist]\n","\n","#drop all entries that contain numbers -> mostly times, dates, room number..\n","filtered_result = [word for word in flat_result if not any(char.isnumeric() for char in word)]\n","\n","text_no_stop=remove_stopwords(filtered_result)\n","text_stemmed=perform_stemming(text_no_stop)\n","\n","\n","# Only keep word that exist in the english lanugage\n","#english_words = [word for word in text_no_stop if is_english_word(word)]\n","\n","#text=english_words\n","\n","text=text_stemmed\n","\n","\n","\n","\n","\n","\n","for i in range(2, len(text) - 2):\n","    context = [text[i - 2], text[i - 1],\n","              text[i + 1], text[i + 2]]\n","    target = text[i]\n","    data.append((context, target))\n","\n","\n","unique_words = set()\n","\n","for item in data:\n","    words, _ = item\n","    unique_words.update(words)\n","\n","# Convert the set to a list to display the unique vocabulary\n","vocab = unique_words\n","vocab_size = len(vocab)\n","\n","\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","\n","\n","X, y = [], []\n","for context, target in data:\n","  X.append(context)\n","  y.append(target)\n","\n","\n","X_numeric=[]\n","y_numeric=[]\n","for i in X:\n","  X_numeric.append([word_to_ix[w] for w in i])\n","print(X_numeric)\n","X_train=torch.FloatTensor(X_numeric)\n","\n","y_numeric=[]\n","for i in y:\n","  y_numeric.append(word_to_ix[i])\n","print(y_numeric)\n","y_train=torch.FloatTensor(y_numeric)\n","\n","\n","\n","\n","num_batches=3000\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","class MyDataset(IterableDataset):\n","    def __init__(self, data_X, data_y):\n","        assert len(data_X) == len(data_y)\n","        self.data_X = data_X.to(device)\n","        self.data_y = data_y.to(device)\n","\n","    def __len__(self):\n","        return len(self.data_X)\n","\n","    def __iter__(self):\n","        for i in range(len(self.data_X)):\n","            yield (self.data_X[i], self.data_y[i])\n","\n","train_set = MyDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=num_batches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6M2pL05aCgzk","executionInfo":{"status":"ok","timestamp":1697720995595,"user_tz":-120,"elapsed":249104,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"25897958-9838-4d84-ddfe-decc51a05505"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["march  all stories new and complete publisher editor if is published bimonthly by quinn publishing company inc kingston new york volume  no  copyright  by quinn publishing company inc application for entry as second class matter at post office buffalo new york pending subscription  for  issues in us and possessions canada  for  issues elsewhere  aiiow four weeks for change of address all stories a\n"]},{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["#load model\n","ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n","word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n","\n","model_save_name=\"scify_CBOW2.pth\"\n","path = f\"/content/drive/MyDrive/'){model_save_name}\"\n","class CBOW(nn.Module):\n","\n","   def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(embedding_dim, 200)\n","        self.linear2 = nn.Linear(200, vocab_size)\n","\n","        self.nonlin_1 = nn.ReLU()\n","\n","        self.nonlin_2 = nn.LogSoftmax(dim = -1)\n","\n","\n","   def forward(self, inputs):\n","      # ask in the forum -> do we need to do the mean or sum\n","        #print(self.embeddings(inputs).shape)\n","        #embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n","        print(inputs.shape)\n","        embeds = self.embeddings(inputs).sum(dim=0)\n","\n","        out = self.linear1(embeds)\n","\n","        out = self.nonlin_1(out)\n","        out =  self.linear2(out)\n","        out = self.nonlin_2(out)\n","\n","        return out\n","\n","# Load the trained model\n","\n","model_path = os.path.join(path)  # Replace with the path to your saved model\n","model = CBOW(vocab_size, 50)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","model.eval()  # Set the model to evaluation mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZbSvE63C2LC","executionInfo":{"status":"ok","timestamp":1697725983778,"user_tz":-120,"elapsed":907,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"662040e4-e9ba-4edc-b204-336296736b5c"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CBOW(\n","  (embeddings): Embedding(218662, 50)\n","  (linear1): Linear(in_features=50, out_features=200, bias=True)\n","  (linear2): Linear(in_features=200, out_features=218662, bias=True)\n","  (nonlin_1): ReLU()\n","  (nonlin_2): LogSoftmax(dim=-1)\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Get closest neighbors for 9 words\n","for i in ['time', 'room', 'dogfish','new','old', 'goddamned','take','fancies','examine']:\n","  print(i)\n","  print(get_closest_word(i, model)[:10])\n","  print('-----------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbQ6Q2u9Eww1","executionInfo":{"status":"ok","timestamp":1697726225357,"user_tz":-120,"elapsed":240595,"user":{"displayName":"Niklas S","userId":"13395037471783967471"}},"outputId":"d5b27417-23bf-4be8-8845-a6401468074d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["time\n","[('profitable', 6.120347499847412), ('letsteve', 6.9447760581970215), ('ganymedeans', 6.966808319091797), ('smnething', 7.000274181365967), ('evhr', 7.019571304321289), ('projjose', 7.056467533111572), ('gondoliers', 7.0955610275268555), ('thass', 7.0956339836120605), ('spaceport', 7.123833656311035), ('rockmold', 7.132011890411377)]\n","-----------------------------------\n","room\n","[('paulines', 5.7859296798706055), ('artery', 5.852158069610596), ('energa', 5.993887424468994), ('barbar', 6.014967918395996), ('passra', 6.074892044067383), ('orbita', 6.076498508453369), ('ceentreels', 6.077091693878174), ('nny', 6.079671859741211), ('headaches', 6.112804889678955), ('therere', 6.123912334442139)]\n","-----------------------------------\n","dogfish\n","[('tribute', 6.2036261558532715), ('groundmoving', 6.207232475280762), ('milkless', 6.3910088539123535), ('hucinatory', 6.394628047943115), ('punchboards', 6.4385576248168945), ('svith', 6.442418098449707), ('mconfets', 6.467662334442139), ('cadmium', 6.510450839996338), ('cbmprehend', 6.522007942199707), ('craterdented', 6.524386405944824)]\n","-----------------------------------\n","new\n","[('bodily', 6.213312149047852), ('beriflame', 6.347684860229492), ('marcti', 6.372077465057373), ('justifigble', 6.389866828918457), ('quimby', 6.440420627593994), ('feekthe', 6.476483345031738), ('gleep', 6.4864091873168945), ('slavequeen', 6.499608039855957), ('granduncles', 6.518282890319824), ('biodetector', 6.552896976470947)]\n","-----------------------------------\n","old\n","[('gorbals', 5.552054405212402), ('pjxidis', 5.783786296844482), ('fiaten', 5.866009712219238), ('composite', 6.005516529083252), ('enktiqnal', 6.044464111328125), ('shielded', 6.070196151733398), ('twohands', 6.074044704437256), ('jackedup', 6.086695671081543), ('inhibiting', 6.12490701675415), ('outlandishsounding', 6.134536266326904)]\n","-----------------------------------\n","goddamned\n","[('bewitching', 6.3184709548950195), ('antiaut', 6.340568542480469), ('slmiliarlty', 6.459449291229248), ('hett', 6.61506462097168), ('kanokoe', 6.64906644821167), ('jimmyll', 6.656421184539795), ('completes', 6.6599884033203125), ('midsister', 6.676921367645264), ('liove', 6.71300745010376), ('dehydrofreezing', 6.716789722442627)]\n","-----------------------------------\n","take\n","[('hoked', 6.509081840515137), ('oneoone', 6.606448173522949), ('saucily', 6.6999192237854), ('nayou', 6.761160373687744), ('laodwork', 6.771914482116699), ('noncity', 6.780378341674805), ('aggre', 6.785505294799805), ('usque', 6.833173751831055), ('tnam', 6.835919380187988), ('mischie', 6.838875770568848)]\n","-----------------------------------\n","fancies\n","[('tradings', 6.21999979019165), ('almness', 6.273158550262451), ('recoile', 6.305172920227051), ('degroot', 6.357814788818359), ('f', 6.384528160095215), ('roblems', 6.422006130218506), ('alexandras', 6.486988067626953), ('tanaquil', 6.486991882324219), ('riving', 6.537322998046875), ('kwith', 6.5389251708984375)]\n","-----------------------------------\n","examine\n","[('glog', 5.898222923278809), ('unequallysjzed', 6.030141830444336), ('artificially', 6.0430803298950195), ('tractorversustractor', 6.070555210113525), ('ydtf', 6.080045700073242), ('wolfes', 6.1375412940979), ('anzin', 6.192697048187256), ('hashimoto', 6.206279754638672), ('trampolines', 6.227177143096924), ('isneeded', 6.244344711303711)]\n","-----------------------------------\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1c8qwzhh_hX9eDAdCx1mc9rR-WXWzHfmD","timestamp":1697470173544},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/word_embeddings_tutorial.ipynb","timestamp":1633945586782}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":0}